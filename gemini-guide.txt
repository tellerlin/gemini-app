
Veo 3 is now available in the Gemini API! Learn more

    Home
    Gemini API
    Models

Gemini models

2.5 Pro

Our most powerful thinking model with maximum response accuracy and state-of-the-art performance

    Input audio, images, video, and text, get text responses
    Tackle difficult problems, analyze large databases, and more
    Best for complex coding, reasoning, and multimodal understanding

2.5 Flash

Our best model in terms of price-performance, offering well-rounded capabilities.

    Input audio, images, video, and text, and get text responses
    Model thinks as needed; or, you can configure a thinking budget
    Best for low latency, high volume tasks that require thinking

2.5 Flash-Lite

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

    Input audio, images, video, and text, and get text responses
    Most cost-efficient model supporting high throughput
    Best for real time, low latency use cases

Note: Gemini 2.5 Pro and 2.5 Flash come with thinking on by default. If you're migrating from a non-thinking model such as 2.0 Pro or Flash, we recommend you to review the Thinking guide first.
Model variants

The Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:
Model variant 	Input(s) 	Output 	Optimized for
Gemini 2.5 Pro
gemini-2.5-pro 	Audio, images, videos, text, and PDF 	Text 	Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more
Gemini 2.5 Flash
gemini-2.5-flash 	Audio, images, videos, and text 	Text 	Adaptive thinking, cost efficiency
Gemini 2.5 Flash-Lite
gemini-2.5-flash-lite 	Text, image, video, audio 	Text 	Most cost-efficient model supporting high throughput
Gemini 2.5 Flash Live
gemini-live-2.5-flash-preview 	Audio, video, and text 	Text, audio 	Low-latency bidirectional voice and video interactions
Gemini 2.5 Flash Native Audio
gemini-2.5-flash-preview-native-audio-dialog &
gemini-2.5-flash-exp-native-audio-thinking-dialog 	Audio, videos, and text 	Text and audio, interleaved 	High quality, natural conversational audio outputs, with or without thinking
Gemini 2.5 Flash Preview TTS
gemini-2.5-flash-preview-tts 	Text 	Audio 	Low latency, controllable, single- and multi-speaker text-to-speech audio generation
Gemini 2.5 Pro Preview TTS
gemini-2.5-pro-preview-tts 	Text 	Audio 	Low latency, controllable, single- and multi-speaker text-to-speech audio generation

 Text generation

The Gemini API can generate text output from various inputs, including text, images, video, and audio, leveraging Gemini models.

Here's a basic example that takes a single text input:
Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();

Thinking with Gemini 2.5

2.5 Flash and Pro models have "thinking" enabled by default to enhance quality, which may take longer to run and increase token usage.

When using 2.5 Flash, you can disable thinking by setting the thinking budget to zero.

For more details, see the thinking guide.
Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();

System instructions and other configurations

You can guide the behavior of Gemini models with system instructions. To do so, pass a GenerateContentConfig object.
Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();

The GenerateContentConfig object also lets you override default generation parameters, such as temperature.
Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();

Refer to the GenerateContentConfig in our API reference for a complete list of configurable parameters and their descriptions.
Multimodal inputs

The Gemini API supports multimodal inputs, allowing you to combine text with media files. The following example demonstrates providing an image:
Python
JavaScript
Go
REST
Apps Script

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();

For alternative methods of providing images and more advanced image processing, see our image understanding guide. The API also supports document, video, and audio inputs and understanding.
Streaming responses

By default, the model returns a response only after the entire generation process is complete.

For more fluid interactions, use streaming to receive GenerateContentResponse instances incrementally as they're generated.
Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();

Multi-turn conversations (Chat)

Our SDKs provide functionality to collect multiple rounds of prompts and responses into a chat, giving you an easy way to keep track of the conversation history.
Note: Chat functionality is only implemented as part of the SDKs. Behind the scenes, it still uses the generateContent API. For multi-turn conversations, the full conversation history is sent to the model with each follow-up turn.
Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();

Streaming can also be used for multi-turn conversations.
Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();

Supported models

All models in the Gemini family support text generation. To learn more about the models and their capabilities, visit the Models page.
Best practices
Prompting tips

For basic text generation, a zero-shot prompt often suffices without needing examples, system instructions or specific formatting.

For more tailored outputs:

    Use System instructions to guide the model.
    Provide few example inputs and outputs to guide the model. This is often referred to as few-shot prompting.

Consult our prompt engineering guide for more tips.
Structured output
In some cases, you may need structured output, such as JSON. Refer to our structured output guide to learn how.

 Generate images using Imagen

Imagen is Google's high-fidelity image generation model, capable of generating realistic and high quality images from text prompts. All generated images include a SynthID watermark. To learn more about the available Imagen model variants, see the Model versions section.
Note: You can also generate images with Gemini's built-in multimodal capabilities. See the Image generation guide for details.
Generate images using the Imagen models

This example demonstrates generating images with an Imagen model:
Python
JavaScript
Go
REST

import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-001',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();

AI-generated image of a robot holding a red skateboard
AI-generated image of a robot holding a red skateboard
Imagen configuration

Imagen supports English only prompts at this time and the following parameters:
Note: Naming conventions of parameters vary by programming language.

    numberOfImages: The number of images to generate, from 1 to 4 (inclusive). The default is 4.
    sampleImageSize: The size of the generated image. This is only supported for the Standard and Ultra models. The supported values are 1K and 2K. Default is 1K.
    aspectRatio: Changes the aspect ratio of the generated image. Supported values are "1:1", "3:4", "4:3", "9:16", and "16:9". The default is "1:1".

    personGeneration: Allow the model to generate images of people. The following values are supported:
        "dont_allow": Block generation of images of people.
        "allow_adult": Generate images of adults, but not children. This is the default.
        "allow_all": Generate images that include adults and children.
    Note: The "allow_all" parameter value is not allowed in EU, UK, CH, MENA locations.

Imagen prompt guide

This section of the Imagen guide shows you how modifying a text-to-image prompt can produce different results, along with examples of images you can create.
Prompt writing basics
Note: Maximum prompt length is 480 tokens.

A good prompt is descriptive and clear, and makes use of meaningful keywords and modifiers. Start by thinking of your subject, context, and style.
Prompt with subject, context, and style emphasized
Image text: A sketch (style) of a modern apartment building (subject) surrounded by skyscrapers (context and background).

    Subject: The first thing to think about with any prompt is the subject: the object, person, animal, or scenery you want an image of.

    Context and background: Just as important is the background or context in which the subject will be placed. Try placing your subject in a variety of backgrounds. For example, a studio with a white background, outdoors, or indoor environments.

    Style: Finally, add the style of image you want. Styles can be general (painting, photograph, sketches) or very specific (pastel painting, charcoal drawing, isometric 3D). You can also combine styles.

After you write a first version of your prompt, refine your prompt by adding more details until you get to the image that you want. Iteration is important. Start by establishing your core idea, and then refine and expand upon that core idea until the generated image is close to your vision.
photorealistic sample image 1
Prompt: A park in the spring next to a lake
	
photorealistic sample image 2
Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour
	
photorealistic sample image 3
Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour, red wildflowers

Imagen models can transform your ideas into detailed images, whether your prompts are short or long and detailed. Refine your vision through iterative prompting, adding details until you achieve the perfect result.

Short prompts let you generate an image quickly.
Imagen 3 short prompt example
Prompt: close-up photo of a woman in her 20s, street photography, movie still, muted orange warm tones
	

Longer prompts let you add specific details and build your image.
Imagen 3 long prompt example
Prompt: captivating photo of a woman in her 20s utilizing a street photography style. The image should look like a movie still with muted orange warm tones.

Additional advice for Imagen prompt writing:

    Use descriptive language: Employ detailed adjectives and adverbs to paint a clear picture for Imagen.
    Provide context: If necessary, include background information to aid the AI's understanding.
    Reference specific artists or styles: If you have a particular aesthetic in mind, referencing specific artists or art movements can be helpful.
    Use prompt engineering tools: Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results.
    Enhancing the facial details in your personal and group images: Specify facial details as a focus of the photo (for example, use the word "portrait" in the prompt).

Generate text in images

Imagen models can add text into images, opening up more creative image generation possibilities. Use the following guidance to get the most out of this feature:

    Iterate with confidence: You might have to regenerate images until you achieve the look you want. Imagen's text integration is still evolving, and sometimes multiple attempts yield the best results.
    Keep it short: Limit text to 25 characters or less for optimal generation.

    Multiple phrases: Experiment with two or three distinct phrases to provide additional information. Avoid exceeding three phrases for cleaner compositions.
    Imagen 3 generate text example
    Prompt: A poster with the text "Summerland" in bold font as a title, underneath this text is the slogan "Summer never felt so good"

    Guide Placement: While Imagen can attempt to position text as directed, expect occasional variations. This feature is continually improving.

    Inspire font style: Specify a general font style to subtly influence Imagen's choices. Don't rely on precise font replication, but expect creative interpretations.

    Font size: Specify a font size or a general indication of size (for example, small, medium, large) to influence the font size generation.

Prompt parameterization

To better control output results, you might find it helpful to parameterize the inputs into Imagen. For example, suppose you want your customers to be able to generate logos for their business, and you want to make sure logos are always generated on a solid color background. You also want to limit the options that the client can select from a menu.

In this example, you can create a parameterized prompt similar to the following:

A {logo_style} logo for a {company_area} company on a solid color background. Include the text {company_name}.

In your custom user interface, the customer can input the parameters using a menu, and their chosen value populates the prompt Imagen receives.

For example:

    Prompt: A minimalist logo for a health care company on a solid color background. Include the text Journey.

    Imagen 3 prompt parameterization example 1

    Prompt: A modern logo for a software company on a solid color background. Include the text Silo.

    Imagen 3 prompt parameterization example 2

    Prompt: A traditional logo for a baking company on a solid color background. Include the text Seed.

    Imagen 3 prompt parameterization example 3

Advanced prompt writing techniques

Use the following examples to create more specific prompts based on attributes like photography descriptors, shapes and materials, historical art movements, and image quality modifiers.
Photography

    Prompt includes: "A photo of..."

To use this style, start with using keywords that clearly tell Imagen that you're looking for a photograph. Start your prompts with "A photo of. . .". For example:
photorealistic sample image 1
Prompt: A photo of coffee beans in a kitchen on a wooden surface
	
photorealistic sample image 2
Prompt: A photo of a chocolate bar on a kitchen counter
	
photorealistic sample image 3
Prompt: A photo of a modern building with water in the background

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.
Photography modifiers

In the following examples, you can see several photography-specific modifiers and parameters. You can combine multiple modifiers for more precise control.

    Camera Proximity - Close up, taken from far away

    close up camera sample image
    Prompt: A close-up photo of coffee beans
    	
    zoomed out camera sample image
    Prompt: A zoomed out photo of a small bag of
    coffee beans in a messy kitchen

    Camera Position - aerial, from below
    aerial photo sample image
    Prompt: aerial photo of urban city with skyscrapers
    	
    a view from underneath sample image
    Prompt: A photo of a forest canopy with blue skies from below

    Lighting - natural, dramatic, warm, cold
    natural lighting sample image
    Prompt: studio photo of a modern arm chair, natural lighting
    	
    dramatic lighting sample image
    Prompt: studio photo of a modern arm chair, dramatic lighting

    Camera Settings - motion blur, soft focus, bokeh, portrait
    motion blur sample image
    Prompt: photo of a city with skyscrapers from the inside of a car with motion blur
    	
    soft focus sample image
    Prompt: soft focus photograph of a bridge in an urban city at night

    Lens types - 35mm, 50mm, fisheye, wide angle, macro
    macro lens sample image
    Prompt: photo of a leaf, macro lens
    	
    fisheye lens sample image
    Prompt: street photography, new york city, fisheye lens

    Film types - black and white, polaroid
    polaroid photo sample image
    Prompt: a polaroid portrait of a dog wearing sunglasses
    	
    black and white photo sample image
    Prompt: black and white photo of a dog wearing sunglasses

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.
Illustration and art

    Prompt includes: "A painting of...", "A sketch of..."

Art styles vary from monochrome styles like pencil sketches, to hyper-realistic digital art. For example, the following images use the same prompt with different styles:

"An [art style or creation technique] of an angular sporty electric sedan with skyscrapers in the background"
art sample images
Prompt: A technical pencil drawing of an angular...
	
art sample images
Prompt: A charcoal drawing of an angular...
	
art sample images
Prompt: A color pencil drawing of an angular...
art sample images
Prompt: A pastel painting of an angular...
	
art sample images
Prompt: A digital art of an angular...
	
art sample images
Prompt: An art deco (poster) of an angular...

Image source: Each image was generated using its corresponding text prompt with the Imagen 2 model.
Shapes and materials

    Prompt includes: "...made of...", "...in the shape of..."

One of the strengths of this technology is that you can create imagery that is otherwise difficult or impossible. For example, you can recreate your company logo in different materials and textures.
shapes and materials example image 1
Prompt: a duffle bag made of cheese
	
shapes and materials example image 2
Prompt: neon tubes in the shape of a bird
	
shapes and materials example image 3
Prompt: an armchair made of paper, studio photo, origami style

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.
Historical art references

    Prompt includes: "...in the style of..."

Certain styles have become iconic over the years. The following are some ideas of historical painting or art styles that you can try.

"generate an image in the style of [art period or movement] : a wind farm"
impressionism example image
Prompt: generate an image in the style of an impressionist painting: a wind farm
	
renaissance example image
Prompt: generate an image in the style of a renaissance painting: a wind farm
	
pop art example image
Prompt: generate an image in the style of pop art: a wind farm

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.
Image quality modifiers

Certain keywords can let the model know that you're looking for a high-quality asset. Examples of quality modifiers include the following:

    General Modifiers - high-quality, beautiful, stylized
    Photos - 4K, HDR, Studio Photo
    Art, Illustration - by a professional, detailed

The following are a few examples of prompts without quality modifiers and the same prompt with quality modifiers.
corn example image without modifiers
Prompt (no quality modifiers): a photo of a corn stalk
	
corn example image with modifiers
Prompt (with quality modifiers): 4k HDR beautiful
photo of a corn stalk taken by a
professional photographer

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.
Aspect ratios

Imagen image generation lets you set five distinct image aspect ratios.

    Square (1:1, default) - A standard square photo. Common uses for this aspect ratio include social media posts.

    Fullscreen (4:3) - This aspect ratio is commonly used in media or film. It is also the dimensions of most old (non-widescreen) TVs and medium format cameras. It captures more of the scene horizontally (compared to 1:1), making it a preferred aspect ratio for photography.
    aspect ratio example
    Prompt: close up of a musician's fingers playing the piano, black and white film, vintage (4:3 aspect ratio)
    	
    aspect ratio example
    Prompt: A professional studio photo of french fries for a high end restaurant, in the style of a food magazine (4:3 aspect ratio)

    Portrait full screen (3:4) - This is the fullscreen aspect ratio rotated 90 degrees. This lets to capture more of the scene vertically compared to the 1:1 aspect ratio.
    aspect ratio example
    Prompt: a woman hiking, close of her boots reflected in a puddle, large mountains in the background, in the style of an advertisement, dramatic angles (3:4 aspect ratio)
    	
    aspect ratio example
    Prompt: aerial shot of a river flowing up a mystical valley (3:4 aspect ratio)

    Widescreen (16:9) - This ratio has replaced 4:3 and is now the most common aspect ratio for TVs, monitors, and mobile phone screens (landscape). Use this aspect ratio when you want to capture more of the background (for example, scenic landscapes).
    aspect ratio example
    Prompt: a man wearing all white clothing sitting on the beach, close up, golden hour lighting (16:9 aspect ratio)

    Portrait (9:16) - This ratio is widescreen but rotated. This a relatively new aspect ratio that has been popularized by short form video apps (for example, YouTube shorts). Use this for tall objects with strong vertical orientations such as buildings, trees, waterfalls, or other similar objects.
    aspect ratio example
    Prompt: a digital render of a massive skyscraper, modern, grand, epic with a beautiful sunset in the background (9:16 aspect ratio)

Photorealistic images

Different versions of the image generation model might offer a mix of artistic and photorealistic output. Use the following wording in prompts to generate more photorealistic output, based on the subject you want to generate.
Note: Take these keywords as general guidance when you try to create photorealistic images. They aren't required to achieve your goal.
Use case 	Lens type 	Focal lengths 	Additional details
People (portraits) 	Prime, zoom 	24-35mm 	black and white film, Film noir, Depth of field, duotone (mention two colors)
Food, insects, plants (objects, still life) 	Macro 	60-105mm 	High detail, precise focusing, controlled lighting
Sports, wildlife (motion) 	Telephoto zoom 	100-400mm 	Fast shutter speed, Action or movement tracking
Astronomical, landscape (wide-angle) 	Wide-angle 	10-24mm 	Long exposure times, sharp focus, long exposure, smooth water or clouds
Portraits
Use case 	Lens type 	Focal lengths 	Additional details
People (portraits) 	Prime, zoom 	24-35mm 	black and white film, Film noir, Depth of field, duotone (mention two colors)

Using several keywords from the table, Imagen can generate the following portraits:
portrait photography example 	portrait photography example 	portrait photography example 	portrait photography example

Prompt: A woman, 35mm portrait, blue and grey duotones
Model: imagen-3.0-generate-002
portrait photography example 	portrait photography example 	portrait photography example 	portrait photography example

Prompt: A woman, 35mm portrait, film noir
Model: imagen-3.0-generate-002
Objects
Use case 	Lens type 	Focal lengths 	Additional details
Food, insects, plants (objects, still life) 	Macro 	60-105mm 	High detail, precise focusing, controlled lighting

Using several keywords from the table, Imagen can generate the following object images:
object photography example 	object photography example 	object photography example 	object photography example

Prompt: leaf of a prayer plant, macro lens, 60mm
Model: imagen-3.0-generate-002
object photography example 	object photography example 	object photography example 	object photography example

Prompt: a plate of pasta, 100mm Macro lens
Model: imagen-3.0-generate-002
Motion
Use case 	Lens type 	Focal lengths 	Additional details
Sports, wildlife (motion) 	Telephoto zoom 	100-400mm 	Fast shutter speed, Action or movement tracking

Using several keywords from the table, Imagen can generate the following motion images:
motion photography example 	motion photography example 	motion photography example 	motion photography example

Prompt: a winning touchdown, fast shutter speed, movement tracking
Model: imagen-3.0-generate-002
motion photography example 	motion photography example 	motion photography example 	motion photography example

Prompt: A deer running in the forest, fast shutter speed, movement tracking
Model: imagen-3.0-generate-002
Wide-angle
Use case 	Lens type 	Focal lengths 	Additional details
Astronomical, landscape (wide-angle) 	Wide-angle 	10-24mm 	Long exposure times, sharp focus, long exposure, smooth water or clouds

Using several keywords from the table, Imagen can generate the following wide-angle images:
wide-angle photography example 	wide-angle photography example 	wide-angle photography example 	wide-angle photography example

Prompt: an expansive mountain range, landscape wide angle 10mm
Model: imagen-3.0-generate-002
wide-angle photography example 	wide-angle photography example 	wide-angle photography example 	wide-angle photography example

Prompt: a photo of the moon, astro photography, wide angle 10mm
Model: imagen-3.0-generate-002
Model versions
Imagen 4
Property 	Description
Model code 	

Gemini API

imagen-4.0-generate-001
imagen-4.0-ultra-generate-001
imagen-4.0-fast-generate-001
Supported data types 	

Input

Text

Output

Images
Token limits[*] 	

Input token limit

480 tokens (text)

Output images

1 to 4 (Ultra/Standard/Fast)
Latest update 	June 2025
Imagen 3
Property 	Description
Model code 	

Gemini API

imagen-3.0-generate-002
Supported data types 	

Input

Text

Output

Images
Token limits[*] 	

Input token limit

N/A

Output images

Up to 4
Latest update 	February 2025


Veo 3 is now available in the Gemini API! Learn more

    Home
    Gemini API
    Models

Image generation with Gemini

Gemini can generate and process images conversationally. You can prompt Gemini with text, images, or a combination of both to achieve various image-related tasks, such as image generation and editing. All generated images include a SynthID watermark.

Image generation may not be available in all regions and countries, review our Gemini models page for more information.
Note: You can also generate images with Imagen, our specialized image generation model. See the When to use Imagen section for details on how to choose between Gemini and Imagen.
Image generation (text-to-image)

The following code demonstrates how to generate an image based on a descriptive prompt. You must include responseModalities: ["TEXT", "IMAGE"] in your configuration. Image-only output is not supported with these models.
Python
JavaScript
Go
REST
Note: We've released the Google SDK for TypeScript and JavaScript in preview launch stage. Use this SDK for image generation features.

import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const contents =
    "Hi, can you create a 3d rendered image of a pig " +
    "with wings and a top hat flying over a happy " +
    "futuristic scifi city with lots of greenery?";

  // Set responseModalities to include "Image" so the model can generate  an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();

AI-generated image of a fantastical flying pig
AI-generated image of a fantastical flying pig
Image editing (text-and-image-to-image)

To perform image editing, add an image as input. The following example demonstrates uploading base64 encoded images. For multiple images and larger payloads, check the image input section.
Python
JavaScript
Go
REST
Note: We've released the Google SDK for TypeScript and JavaScript in preview launch stage. Use this SDK for image generation features.

import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  // Load the image from the local file system
  const imagePath = "path/to/image.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  // Prepare the content parts
  const contents = [
    { text: "Can you add a llama next to the image?" },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  // Set responseModalities to include "Image" so the model can generate an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();

Other image generation modes

Gemini supports other image interaction modes based on prompt structure and context, including:

    Text to image(s) and text (interleaved): Outputs images with related text.
        Example prompt: "Generate an illustrated recipe for a paella."
    Image(s) and text to image(s) and text (interleaved): Uses input images and text to create new related images and text.
        Example prompt: (With an image of a furnished room) "What other color sofas would work in my space? can you update the image?"
    Multi-turn image editing (chat): Keep generating / editing images conversationally.
        Example prompts: [upload an image of a blue car.] , "Turn this car into a convertible.", "Now change the color to yellow."

Limitations

    For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.
    Image generation does not support audio or video inputs.
    Image generation may not always trigger:
        The model may output text only. Try asking for image outputs explicitly (e.g. "generate an image", "provide images as you go along", "update the image").
        The model may stop generating partway through. Try again or try a different prompt.
    When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.
    There are some regions/countries where Image generation is not available. See Models for more information.

When to use Imagen

In addition to using Gemini's built-in image generation capabilities, you can also access Imagen, our specialized image generation model, through the Gemini API.

Choose Gemini when:

    You need contextually relevant images that leverage world knowledge and reasoning.
    Seamlessly blending text and images is important.
    You want accurate visuals embedded within long text sequences.
    You want to edit images conversationally while maintaining context.

Choose Imagen when:

    Image quality, photorealism, artistic detail, or specific styles (e.g., impressionism, anime) are top priorities.
    Performing specialized editing tasks like product background updates or image upscaling.
    Infusing branding, style, or generating logos and product designs.

Imagen 4 should be your go-to model starting to generate images with Imagen. Choose Imagen 4 Ultra for advanced use-cases or when you need the best image quality. Note that Imagen 4 Ultra can only generate one image at a time.

 Grounding with Google Search

Grounding with Google Search connects the Gemini model to real-time web content and works with all available languages. This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.

Grounding helps you build applications that can:

    Increase factual accuracy: Reduce model hallucinations by basing responses on real-world information.
    Access real-time information: Answer questions about recent events and topics.

    Provide citations: Build user trust by showing the sources for the model's claims.

 URL context

Experimental: The URL context tool is an experimental feature.

Using the URL context tool, you can provide Gemini with URLs as additional context for your prompt. The model can then retrieve content from the URLs that is not behind a paywall and use that content to inform and shape its response.

This tool is useful for tasks like the following:

    Extracting key data points or talking points from articles
    Comparing information across multiple links
    Synthesizing data from several sources
    Answering questions based on the content of a specific page or pages
    Analyzing content for specific purposes (like writing a job description or creating test questions)

This guide explains how to use the URL context tool in the Gemini API.
Use URL context

You can use the URL context tool in two main ways, by itself or in conjunction with Grounding with Google Search.

URL Context Only

You provide specific URLs that you want the model to analyze directly in your prompt.

Example prompts:

Summarize this document: YOUR_URLs

Extract the key features from the product description on this page: YOUR_URLs

Grounding with Google Search + URL Context

You can also enable both URL context and Grounding with Google Search together. You can enter a prompt with or without URLs. The model may first search for relevant information and then use the URL context tool to read the content of the search results for a more in-depth understanding.

Example prompts:

Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.

Recommend 3 books for beginners to read to learn more about the latest YOUR_subject.

Code examples with URL context only
Python
Javascript
REST

from google import genai
from google.genai.types import Tool, GenerateContentConfig, UrlContext

client = genai.Client()
model_id = "gemini-2.5-flash"

url_context_tool = Tool(
    url_context = UrlContext
)

response = client.models.generate_content(
    model=model_id,
    contents="Compare recipes from YOUR_URL1 and YOUR_URL2",
    config=GenerateContentConfig(
        tools=[url_context_tool],
        response_modalities=["TEXT"],
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)

Code examples with Grounding with Google Search
Python
Javascript
REST

from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch, UrlContext

client = genai.Client()
model_id = "gemini-2.5-flash"

tools = []
tools.append(Tool(url_context=UrlContext))
tools.append(Tool(google_search=GoogleSearch))

response = client.models.generate_content(
    model=model_id,
    contents="Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    config=GenerateContentConfig(
        tools=tools,
        response_modalities=["TEXT"],
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)

For more details about Grounding with Google Search, see the overview page.
Contextual response

The model's response will be based on the content it retrieved from the URLs. If the model retrieved content from URLs, the response will include url_context_metadata. Such a response might look something like the following (parts of the response have been omitted for brevity):

{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "... \n"
          }
        ],
        "role": "model"
      },
      ...
      "url_context_metadata":
      {
          "url_metadata":
          [
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/1234567890abcdef",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/abcdef1234567890",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "YOUR_URL",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/fedcba0987654321",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            }
          ]
        }
    }
}

Supported models

    gemini-2.5-pro
    gemini-2.5-flash
    gemini-2.5-flash-lite
    gemini-2.0-flash
    gemini-2.0-flash-live-001

Limitations

    The tool will consume up to 20 URLs per request for analysis.
    The tool does not retrieve content behind paywalls.
    For best results during experimental phase, use the tool on standard web pages rather than multimedia content such as YouTube videos.
    During experimental phase, the tool is free to use. Billing to come later.

    The experimental release has the following quotas:
        1500 queries per day per project for requests made through the Gemini API
        100 queries per day per user in Google AI Studio


 Grounding with Google Search

Grounding with Google Search connects the Gemini model to real-time web content and works with all available languages. This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.

Grounding helps you build applications that can:

    Increase factual accuracy: Reduce model hallucinations by basing responses on real-world information.
    Access real-time information: Answer questions about recent events and topics.

    Provide citations: Build user trust by showing the sources for the model's claims.

Python
JavaScript
REST

import { GoogleGenAI } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Define the grounding tool
const groundingTool = {
  googleSearch: {},
};

// Configure generation settings
const config = {
  tools: [groundingTool],
};

// Make the request
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

// Print the grounded response
console.log(response.text);

You can learn more by trying the Search tool notebook.
How grounding with Google Search works

When you enable the google_search tool, the model handles the entire workflow of searching, processing, and citing information automatically.

grounding-overview

    User Prompt: Your application sends a user's prompt to the Gemini API with the google_search tool enabled.
    Prompt Analysis: The model analyzes the prompt and determines if a Google Search can improve the answer.
    Google Search: If needed, the model automatically generates one or multiple search queries and executes them.
    Search Results Processing: The model processes the search results, synthesizes the information, and formulates a response.
    Grounded Response: The API returns a final, user-friendly response that is grounded in the search results. This response includes the model's text answer and groundingMetadata with the search queries, web results, and citations.

Understanding the Grounding Response

When a response is successfully grounded, the response includes a groundingMetadata field. This structured data is essential for verifying claims and building a rich citation experience in your application.

{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}

The Gemini API returns the following information with the groundingMetadata:

    webSearchQueries : Array of the search queries used. This is useful for debugging and understanding the model's reasoning process.
    searchEntryPoint : Contains the HTML and CSS to render the required Search Suggestions. Full usage requirements are detailed in the Terms of Service.
    groundingChunks : Array of objects containing the web sources (uri and title).
    groundingSupports : Array of chunks to connect model response text to the sources in groundingChunks. Each chunk links a text segment (defined by startIndex and endIndex) to one or more groundingChunkIndices. This is the key to building inline citations.

Grounding with Google Search can also be used in combination with the URL context tool to ground responses in both public web data and the specific URLs you provide.
Attributing Sources with inline Citations

The API returns structured citation data, giving you complete control over how you display sources in your user interface. You can use the groundingSupports and groundingChunks fields to link the model's statements directly to their sources. Here is a common pattern for processing the metadata to create a response with inline, clickable citations.
Python
JavaScript

function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);

The new response with inline citations will look like this:

Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)

Pricing

When you use Grounding with Google Search, your project is billed per API request that includes the google_search tool. If the model decides to execute multiple search queries to answer a single prompt (for example, searching for "UEFA Euro 2024 winner" and "Spain vs England Euro 2024 final score" within the same API call), this counts as a single billable use of the tool for that request.

For detailed pricing information, see the Gemini API pricing page.
Supported Models

Experimental and Preview models are not included. You can find their capabilities on the model overview page.
Model 	Grounding with Google Search
Gemini 2.5 Pro 	✔️
Gemini 2.5 Flash 	✔️
Gemini 2.0 Flash 	✔️
Gemini 1.5 Pro 	✔️
Gemini 1.5 Flash 	✔️
Note: Older models use a google_search_retrieval tool. For all current models, use the google_search tool as shown in the examples.
Grounding with Gemini 1.5 Models (Legacy)

While the google_search tool is recommended for Gemini 2.0 and later, Gemini 1.5 support a legacy tool named google_search_retrieval. This tool provides a dynamic mode that allows the model to decide whether to perform a search based on its confidence that the prompt requires fresh information. If the model's confidence is above a dynamic_threshold you set (a value between 0.0 and 1.0), it will perform a search.
Python
JavaScript
REST

// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}

